{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e1299fc",
   "metadata": {},
   "source": [
    "# Facemask Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896835d4",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "\n",
    "1. [Import Necessary Packages](#import)\n",
    "2. [Preprocess Dataset](#preprocess)\n",
    "3. [Decode Images to Numpy Arrays](#decode)\n",
    "4. [Model Training and Validation](#training)\n",
    "5. [User Camera Input](#camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e34ba",
   "metadata": {},
   "source": [
    "### __Introduction:__\n",
    "### This notebook has been created to demonstrate the backend Python code for the Facemask Detection project. Please make sure all data are downloaded to local machine before running this notebook.  \n",
    "### Follow us on Instagram: https://www.instagram.com/kati.maskdetect/ \n",
    "### Our project website: https://kati-test.herokuapp.com/home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa472c4",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Packages <a name=\"import\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f589388",
   "metadata": {},
   "source": [
    "- pandas framework for reading csv file \n",
    "- numpy for storing decoded images\n",
    "- tensorflow for CNN training\n",
    "- train_test_split for train validation split\n",
    "- cv2 for enabling camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d211019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "# please make sure all packages are installed on local machine\n",
    "# if not, use pip install\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074f9c",
   "metadata": {},
   "source": [
    "## 2. Preprocess Dataset <a name=\"preprocess\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f562ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      filename                 filepath  label\n",
      "0  Image_1.jpg  data/images/Image_1.jpg      0\n",
      "1  Image_2.jpg  data/images/Image_2.jpg      0\n",
      "2  Image_3.jpg  data/images/Image_3.jpg      0\n",
      "3  Image_4.jpg  data/images/Image_4.jpg      0\n",
      "4  Image_5.jpg  data/images/Image_5.jpg      0\n"
     ]
    }
   ],
   "source": [
    "# read the csv label file from local disk\n",
    "labels = pd.read_csv('data/train_labels_cleaned.csv')\n",
    "\n",
    "# select binary labels only and rename them\n",
    "binary_labels = labels[['filename', 'label (0/1)']]\n",
    "label_only = labels[['label (0/1)']]\n",
    "binary_labels = binary_labels.rename(columns={'label (0/1)': 'label'})\n",
    "\n",
    "# concatenate the path name with the file name\n",
    "data_dir = 'data/images/'\n",
    "filepath = [[filename, data_dir + filename] for filename in labels['filename']]\n",
    "\n",
    "# save the paths to a pandas dataframe\n",
    "pd_filepath = pd.DataFrame(filepath, columns=['filename', 'filepath'])\n",
    "\n",
    "# this data table contains file name, file path, and labels\n",
    "data = pd.merge(pd_filepath, binary_labels, how='inner', on='filename')\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac7315",
   "metadata": {},
   "source": [
    "## 3. Decode Images to Numpy Arrays <a name=\"decode\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa64119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an image file to an array\n",
    "def decode_img(img):\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/io/decode_jpeg\n",
    "    # please see the above link for the following function: tf.image.decode_jpeg\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # uniform picture sizes, this parameter can be tuned\n",
    "    return tf.image.resize(img, [100, 100])\n",
    "\n",
    "\n",
    "# x is the array of images\n",
    "x = []\n",
    "# y is the array of labels\n",
    "y = []\n",
    "\n",
    "# iterate through the data table\n",
    "for index, row in data.iterrows():\n",
    "    # read image according the filepath\n",
    "    img = tf.io.read_file(row['filepath'])\n",
    "    # decode, than convert to numpy format\n",
    "    img = decode_img(img).numpy()\n",
    "    # read label\n",
    "    label = row['label']\n",
    "    # append them to the arrays\n",
    "    x.append(img)\n",
    "    y.append(label)\n",
    "\n",
    "# convert x, y to numpy arrays\n",
    "y = np.array(y)\n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca133ad",
   "metadata": {},
   "source": [
    "## 4. Model Training and Validation <a name=\"training\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c15b1023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "282/282 [==============================] - 32s 111ms/step - loss: 0.2660 - accuracy: 0.8902 - val_loss: 0.1433 - val_accuracy: 0.9494\n",
      "Epoch 2/3\n",
      "282/282 [==============================] - 33s 116ms/step - loss: 0.1135 - accuracy: 0.9597 - val_loss: 0.0963 - val_accuracy: 0.9689\n",
      "Epoch 3/3\n",
      "282/282 [==============================] - 33s 118ms/step - loss: 0.0788 - accuracy: 0.9726 - val_loss: 0.0971 - val_accuracy: 0.9694\n",
      "INFO:tensorflow:Assets written to: model_save\\assets\n"
     ]
    }
   ],
   "source": [
    "# train, validation split\n",
    "# no test data since we will use real images as our test data\n",
    "# random_state=42, 42 is the golden standard for random seed\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# model, the layers in this part can be tuned\n",
    "# to improve the model, feel free to adjust/add/delete the following layers\n",
    "model = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.Rescaling(1. / 255),\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# binary crossentropy is a loss function that is used in binary classification tasks\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# epochs can be tuned\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=3)\n",
    "model.save(\"model_save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557d3b9",
   "metadata": {},
   "source": [
    "## 5. User Camera Input <a name=\"camera\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16dc0a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "def camera_input_color(cap):\n",
    "    status = \"\"\n",
    "    while True:\n",
    "        # capture frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "        # use putText() method for inserting text on video\n",
    "        cv2.putText(frame, status, (50, 50), font, 1, (0, 255, 255), 2, cv2.LINE_4)\n",
    "\n",
    "        color = cv2.cvtColor(frame, cv2.COLOR_BGR2BGRA)\n",
    "        # display the frame\n",
    "        cv2.imshow('frame', color)\n",
    "        # save the image continuously on local disk\n",
    "        cv2.imwrite('data/captured_images/c1.jpg', frame)\n",
    "        # read the image\n",
    "        captured_img = tf.io.read_file('data/captured_images/c1.jpg')\n",
    "        # decode the image and store it in a numpy array\n",
    "        captured_img = decode_img(captured_img).numpy()\n",
    "        # create a batch\n",
    "        captured_img = (np.expand_dims(captured_img, 0))\n",
    "        # predict the image\n",
    "        prob = model.predict(captured_img)\n",
    "        # 0.9 can be tuned\n",
    "        if prob[0][0] > 0.9:\n",
    "            status = \"With mask\"\n",
    "        else:\n",
    "            status = \"No mask\"\n",
    "\n",
    "        # turn off the camera on signal \"q\"\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "def release_camera(cap):\n",
    "    # release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "camera_input_color(cap)\n",
    "release_camera(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471a05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
